version: "3"

services:
  jupyter:
    build:
      context: .
      dockerfile: ./containers/pyth/Dockerfile
    ports:
      - "8888:8888" #jupyter
      - "4040:4040" #spark job
    restart: unless-stopped
    command: sh /jupyter_run.sh
  namenode:
    image: bde2020/hadoop-namenode:1.1.0-hadoop2.8-java8
    container_name: namenode
    volumes:
      - ./data/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    ports:
      - 50070:50070
  datanode:
    image: bde2020/hadoop-datanode:1.1.0-hadoop2.8-java8
    depends_on:
      - namenode
    volumes:
      - ./data/datanode:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    ports:
      - 50075:50075
  spark-master:
    image: bde2020/spark-master:2.1.0-hadoop2.8-hive-java8
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    env_file:
      - ./hadoop.env
  spark-worker:
    image: bde2020/spark-worker:2.1.0-hadoop2.8-hive-java8
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - 8081:8081
    env_file:
      - ./hadoop.env
  zeppelin:
    image: dylanmei/zeppelin
    environment:
      ZEPPELIN_PORT: 5000
      ZEPPELIN_JAVA_OPTS: >-
        -Dspark.driver.memory=1g
        -Dspark.executor.memory=2g
      MASTER: local[*]
    ports:
      - 5000:5000
    volumes:
      - ./data:/usr/zeppelin/data
      - ./notebooks:/usr/zeppelin/notebook
  hue:
    image: bde2020/hdfs-filebrowser:3.11
    ports:
      - 8088:8088
    environment:
      - NAMENODE_HOST=namenode
